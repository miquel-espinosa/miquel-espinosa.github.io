<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="The Segment Anything Model (SAM) was originally designed for label-agnostic mask generation. Does this model also possess inherent semantic understanding, of value to broader visual tasks? In this work we follow a multi-staged approach to answer this question.">
  <meta property="og:title" content="There is no SAMantics! Exploring SAM as a Backbone for Visual Understanding Tasks"/>
  <meta property="og:description" content="The Segment Anything Model (SAM) was originally designed for label-agnostic mask generation. Does this model also possess inherent semantic understanding, of value to broader visual tasks? In this work we follow a multi-staged approach to answer this question."/>
  <meta property="og:url" content="https://miquel-espinosa.github.io/samantics"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/banner.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="There is no SAMantics! Exploring SAM as a Backbone for Visual Understanding Tasks">
  <meta name="twitter:description" content="The Segment Anything Model (SAM) was originally designed for label-agnostic mask generation. Does this model also possess inherent semantic understanding, of value to broader visual tasks? In this work we follow a multi-staged approach to answer this question.">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/twitter_banner.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="SAM, SAMantics, Segment Anything Model, semantics, SAM as a backbone, SAM for visual understanding">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>There is no SAMantics! Exploring SAM as a Backbone for Visual Understanding Tasks</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">There is no SAMantics!</h1>
            <h2 class="subtitle is-3">Exploring SAM as a Backbone for Visual Understanding Tasks</h2>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://miquel-espinosa.github.io/" target="_blank">Miguel Espinosa</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="https://chenhongyiyang.com/" target="_blank">Chenhongyi Yang</a><sup>*</sup>,</span>
                  <span class="author-block">
                    <a href="https://linusericsson.github.io/" target="_blank">Linus Ericsson</a>,
                  </span>
                  <span class="author-block">
                    <a href="https://smcdonagh.github.io/" target="_blank">Steven McDonagh</a>,
                  </span>
                  <span class="author-block">
                    <a href="https://elliotjcrowley.github.io/" target="_blank">Elliot J. Crowley</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">University of Edinburgh<br>Arxiv 2024</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/pdf/2411.15288" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/miquel-espinosa/samantics" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2411.15288" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

              <!-- New overview button below -->
              <div class="overview-button-wrapper">
                <button id="openModalBtn" class="button is-normal is-rounded is-dark" style="width: 50%; background-color: #6D9EEB">
                  <span class="icon">
                    <i class="fas fa-image"></i>
                  </span>
                  <span>Visualise Overview Figure</span>
                </button>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Modal Image-->
<div id="imageModal" class="modal">
  <div class="modal-background"></div>
  <div class="modal-content">
    <p class="image">
      <img id="modalImage" src="static/images/teaser.png" alt="Method Overview">
    </p>
  </div>
  <button class="modal-close is-large" aria-label="close"></button>
</div>
<!-- End Modal Image -->

<!-- Teaser video-->
<section class="hero teaser" style="margin-top: -4rem;">
  <div class="container is-max-desktop">
    <div class="hero-body" style="padding-top: 0;">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/samantics.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        The Segment Anything Model (SAM) was originally designed for label-agnostic mask generation. Does this model also possess inherent semantic understanding, of value to broader visual tasks? In this work we follow a multi-staged approach to answer this question.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            The Segment Anything Model (SAM) was originally designed for label-agnostic mask generation. Does this model also possess inherent semantic understanding, of value to broader visual tasks? In this work we follow a multi-staged approach to answer this question. We firstly quantify SAM's semantic capabilities by comparing base image encoder efficacy under classification tasks, in comparison with established models (CLIP and DINOv2). Our findings reveal a significant lack of semantic discriminability in SAM feature representations, limiting potential for tasks that require class differentiation. This initial result motivates our exploratory study that attempts to enable semantic information via in-context learning with lightweight fine-tuning where we observe that generalisability to unseen classes remains limited. Our observations culminate in the proposal of a training-free approach that leverages DINOv2 features, towards better endowing SAM with semantic understanding and achieving instance-level class differentiation through feature-based similarity. Our study suggests that incorporation of external semantic sources provides a promising direction for the enhancement of SAM's utility with respect to complex visual tasks that require semantic understanding.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Quantifying Semantics Section -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3"><span style="background-color: #C9DAF8; color: #363636; padding: 0.2em 0.4em; border-radius: 0.3em">Quantifying</span> Semantics in SAM</h2>
        <div class="content has-text-justified">
          <p>
            We benchmark SAM feature representations against popular vision encoders (CLIP and DINOv2) on image classification tasks, measuring the presence of semantics via linear probing on respective features. Despite impressive segmentation abilities, we find SAM lacks discriminative feature quality to enable successful classification, indicating limited semantic encoding.
          </p>
        </div>

        <!-- Single centered image -->
        <div class="columns is-centered">
          <div class="column is-10">
            <figure class="image">
              <img src="static/images/imagenet-gap.png" alt="ImageNet1K performance gap">
              <figcaption>ImageNet1K performance gap between SAM and CLIP/DINOv2</figcaption>
            </figure>
          </div>
        </div>

        <!-- Results Table -->
        <div class="table-container">
          <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
            <thead>
              <tr>
                <th>Model</th>
                <th>Top-1 Acc. (%)</th>
                <th>Top-5 Acc. (%)</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>SAM</td>
                <td>11.06</td>
                <td>25.37</td>
              </tr>
              <tr>
                <td>SAM 2</td>
                <td>23.16</td>
                <td>44.44</td>
              </tr>
              <tr>
                <td>CLIP</td>
                <td>73.92</td>
                <td>92.89</td>
              </tr>
              <tr>
                <td>DINOv2</td>
                <td>77.43</td>
                <td>93.78</td>
              </tr>
            </tbody>
          </table>
          <figcaption>Image classification accuracy of SAM, SAM 2, CLIP, and DINOv2 on the ImageNet1K dataset using linear probing.</figcaption>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Quantifying Semantics Section -->

<!-- Recovering Semantics Section -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3"><span style="background-color: #FCE5CD; color: #363636; padding: 0.2em 0.4em; border-radius: 0.3em">Recovering</span> Semantics in SAM</h2>
        <div class="content has-text-justified">
          <p>
            We then explore whether SAM's inherent representations can be adapted for semantic tasks through lightweight fine-tuning. By introducing in-context learning with reference images, we help SAM capture class-specific information for more effective segmentation. This approach shows some success but reveals a critical limitation: the adapted model overfits to known classes, struggling to generalise to new ones.
          </p>
        </div>

        <!-- Architecture Image -->
        <div class="columns is-centered">
          <div class="column">
            <figure class="image">
              <img src="static/images/sam-detr.png" alt="Architecture diagram">
              <figcaption>Training pipeline: Reference image and category are encoded by SAM to get token embeddings, which condition a DETR decoder to predict boxes on the target image. These boxes then prompt SAM to generate masks. Only DETR and token-merge MLPs are trained, while SAM remains frozen.</figcaption>
            </figure>
          </div>
        </div>

        <!-- Results Carousel -->
        <div id="results-carousel-2" class="carousel results-carousel">
          <div class="item">
            <img src="static/images/results-1.png" alt="Result 1"/>
            <h2 class="subtitle has-text-centered">
              Example result using 4 reference images
            </h2>
          </div>
          <div class="item">
            <img src="static/images/results-2.png" alt="Result 2"/>
            <h2 class="subtitle has-text-centered">
              Example result using 4 reference images
            </h2>
          </div>
          <div class="item">
            <img src="static/images/results-3.png" alt="Result 3"/>
            <h2 class="subtitle has-text-centered">
              Example result using 4 reference images
            </h2>
          </div>
          <div class="item">
            <img src="static/images/results-4.png" alt="Result 4"/>
            <h2 class="subtitle has-text-centered">
              Example result using 2 reference images
            </h2>
          </div>
          <div class="item">
            <img src="static/images/results-5.png" alt="Result 5"/>
            <h2 class="subtitle has-text-centered">
              Example result using 2 reference images
            </h2>
          </div>
        </div>

        <!-- Failure Cases -->
        <h3 class="title is-4 mt-6">Failure Cases</h3>
        <div class="content has-text-justified">
          <p>
            While our approach shows promise on known classes, it faces significant challenges when encountering novel categories not seen during training. These failure cases highlight a fundamental limitation in the model's ability to generalize semantic understanding beyond its training distribution.
          </p>
        </div>
        <div class="columns is-centered">
          <div class="column is-10">
            <figure class="image">
              <img src="static/images/coco-novel.png" alt="Failure cases">
              <figcaption>In the top example, skateboards from the BASE set are accurately segmented, but persons from the NOVEL set are not. Middle: mouse and keyboard instances from BASE are identified but person and tv from NOVEL are not. Bottom: Trucks from BASE are identified, but car, motorcycle and person from NOVEL are not. </figcaption>
            </figure>
          </div>
        </div>

        <!-- T-SNE Exploration -->
        <h3 class="title is-4 mt-6">T-SNE Exploration</h3>
        <div class="content has-text-justified">
          <p>
            We make use of T-SNE plots to visualise the features, as shown in the figure below. These plots reveal that the class semantic information is not present before the MLP. Instead, class semantics are actually being learned in the MLP layer, therefore overfitting to the classes seen during training. While the MLP layer encodes class information, this semantic structure is absent in the earlier features, highlighting that the semantics that are recovered in the model are specific to the classes we tune on, and that these class-specific cues do not generalise to unseen classes.
          </p>
        </div>
        <div class="columns is-centered">
          <div class="column is-10">
            <figure class="image">
              <img src="static/images/tsne-features-visualisation.png" alt="T-SNE visualization">
              <figcaption>T-SNE visualization of feature space before and after fine-tuning, and comparison with DINOv2 and CLIP features.</figcaption>
            </figure>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Recovering Semantics Section -->

<!-- Injecting External Semantics Section -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3"><span style="background-color: #D9EAD3; color: #363636; padding: 0.2em 0.4em; border-radius: 0.3em">Injecting</span> External Semantics</h2>
        <div class="content has-text-justified">
          <p>
            Motivated by SAM's inherent semantic gap, we experiment with injecting semantics directly from a pretrained, semantically rich model. Using DINOv2 as a backbone, we create a hybrid architecture that fuses SAM's segmentation strengths with external semantic knowledge. Preliminary results suggest a promising direction for the integration of semantic awareness, without exhaustive model retraining.
          </p>
        </div>

        <!-- Results Table -->
        <div class="table-container">
          <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
            <thead>
              <tr>
                <th rowspan="2"></th>
                <th colspan="2">Box</th>
                <th colspan="2">Mask</th>
              </tr>
              <tr>
                <th>AP</th>
                <th>AR</th>
                <th>AP</th>
                <th>AR</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td>SAM+DINOv2</td>
                <td>13.3</td>
                <td>39.9</td>
                <td>11.3</td>
                <td>34.1</td>
              </tr>
            </tbody>
          </table>
          <p class="has-text-centered is-size-7 mt-2">
            Preliminary box and mask AP and AR results on COCO validation split using DINOv2 with SAM. Since it is a training-free method, these results can be interpreted as generalisation to unseen classes.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Injecting External Semantics Section -->

<!-- Conclusion Section -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Conclusion</h2>
        <div class="content has-text-justified">
          <p>
            Our study reveals a significant semantic gap in SAM's feature representations, limiting its utility for tasks that require class differentiation. While SAM's in-context learning approach shows promise, it struggles with generalising to unseen classes. We propose a training-free approach that leverages DINOv2 features to enhance SAM's semantic understanding and enable instance-level class differentiation through feature-based similarity. Our study suggests that incorporating external semantic sources offers a promising direction for the enhancement of SAM's utility with respect to complex visual tasks that require semantic understanding.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End Conclusion Section -->

<!--BibTex citation -->
  <section class="section hero" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{espinosa2024samantics,
        title={There is no SAMantics! Exploring SAM as a Backbone for Visual Understanding Tasks}, 
        author={Miguel Espinosa and Chenhongyi Yang and Linus Ericsson and Steven McDonagh and Elliot J. Crowley},
        year={2024},
        eprint={2405.00000},
        archivePrefix={arXiv},
        primaryClass={cs.CV}
      }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer is-light">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
